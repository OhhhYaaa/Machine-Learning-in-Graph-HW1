{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b45bf55e-7114-4e4f-a12b-040e8a852231",
   "metadata": {},
   "source": [
    "# HW1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0b575d7a-ea09-4b55-bbe5-6fcb1cd51bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, glob, tqdm, time, csv, json\n",
    "from tqdm import tqdm, trange\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "from sklearn import preprocessing\n",
    "import scipy.stats as stats\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import networkx as nx\n",
    "from torch_geometric.data import Data, InMemoryDataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import Linear, Parameter, Sequential\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import degree, add_self_loops, convert\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.utils.convert import to_networkx, from_networkx\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3612f20e-c419-47c4-911a-44a3871f7b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1407bf70-1678-4206-af11-417e85ef3a98",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "193430e8-17c8-4247-b7a3-7ea7006e5276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_init(graph) :\n",
    "    row, col = graph.edge_index\n",
    "    row = row.unsqueeze(0)\n",
    "    col = col.unsqueeze(0)\n",
    "    t = torch.hstack([row, col])\n",
    "    t = t.squeeze()\n",
    "\n",
    "    deg = degree(t).float().unsqueeze(1)\n",
    "    extend = torch.ones(graph.num_nodes, 2)\n",
    "    init = torch.hstack([deg, extend])\n",
    "    graph.x = init\n",
    "    \n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "57578591-5a3e-42e9-8230-146af9bf3bae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data(edge_index=[2, 19982], num_nodes=5000, x=[5000, 3])\n",
      "[9.41745309e-02 5.39707966e-02 4.43436579e-02 ... 1.95420222e-05\n",
      " 6.05783053e-05 1.09075567e-04]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'/home/rita/111/111-2MLG/HW1'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# youtube\n",
    "os.chdir('/home/rita/111/111-2MLG/HW1')\n",
    "youtube_edge = np.array(pd.read_table('./hw1_data/youtube/com-youtube.txt', sep = ' '))\n",
    "youtube_score = np.array(pd.read_table('./hw1_data/youtube/com-youtube_score.txt', header = None))[::, 1]\n",
    "youtube = Data(edge_index = torch.from_numpy(youtube_edge.T), num_nodes = youtube_edge.max() + 1)\n",
    "youtube = set_init(youtube)\n",
    "\n",
    "# 30\n",
    "path = '/home/rita/111/111-2MLG/HW1/hw1_data/Synthetic/5000'\n",
    "os.chdir(path)\n",
    "files = glob.glob('*')\n",
    "file_name = [x[:-4] for x in files]\n",
    "\n",
    "total_data = {}\n",
    "for i in range(len(files)) :\n",
    "    total_data[file_name[i]] = np.array(pd.read_table(files[i], header = None))\n",
    "\n",
    "total_graph = {}\n",
    "for i in range(len(files)//2):\n",
    "    edge = total_data[str(i)]\n",
    "    temp = Data(edge_index =  torch.from_numpy(edge.T), num_nodes = edge.max() + 1)\n",
    "    total_graph[str(i)] = set_init(temp)    \n",
    "\n",
    "files = glob.glob('*score.*')\n",
    "file_name = [x[:-10] for x in files]\n",
    "\n",
    "total_score = {}\n",
    "for i in range(len(file_name)) :\n",
    "    total_score[file_name[i]] = np.array(pd.read_table(files[i], header = None))[::, 1]\n",
    "    \n",
    "os.chdir('/home/rita/111/111-2MLG/HW1')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd2e56a8-cfc6-490c-919f-f449653f0fe9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bcf86be2-77f7-4338-bcb0-f226b4f96848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GCNconv\n",
    "# 修改成 Algorithm 裡面的樣子\n",
    "class GCNConv_self(MessagePassing):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__(aggr='add')  # \"Add\" aggregation (Step 5).\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        row, col = edge_index\n",
    "        row = row.unsqueeze(0)\n",
    "        col = col.unsqueeze(0)\n",
    "        t = torch.hstack([row, col])\n",
    "        t = t.squeeze()\n",
    "        \n",
    "        deg = degree(t, x.size(0), dtype=x.dtype) + 1\n",
    "        deg_inv_sqrt = deg.pow(-0.5)\n",
    "        deg_inv_sqrt[deg_inv_sqrt == float('inf')] = 0\n",
    "        norm = deg_inv_sqrt[row] * deg_inv_sqrt[col]\n",
    "        out = self.propagate(edge_index, x=x, norm=norm)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def message(self, x_j, norm):\n",
    "        return norm.view(-1, 1) * x_j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ded484de-3403-4598-ab00-d72d16810de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC(\n",
      "  (line2): Sequential(\n",
      "    (0): Linear(in_features=3, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "  )\n",
      "  (line5): GCNConv_self()\n",
      "  (line6): GRUCell(128, 128, bias=False)\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=32, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# DrBC\n",
    "# ENCODER DECODER\n",
    "class DrBC(nn.Module) :\n",
    "    def __init__(self, encoder_layers = 5, emb_dim = 128) :\n",
    "        super(DrBC, self).__init__()\n",
    "        # encoder\n",
    "        self.encoder_layers = encoder_layers\n",
    "        self.emb_dim = emb_dim\n",
    "        # self.n_nodes = n_nodes\n",
    "        self.line2 = nn.Sequential(\n",
    "            nn.Linear(3, self.emb_dim),  # w0 (5000 * p)\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.line5 = GCNConv_self(self.emb_dim, self.emb_dim)\n",
    "        # w1, u1, w2, u2, w3, u3  (p * p)\n",
    "        self.line6 = nn.GRUCell(input_size = self.emb_dim, hidden_size = self.emb_dim, bias = False) # (p * p)\n",
    "        \n",
    "        # decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(self.emb_dim, 32),  # w4\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(32, 1),  # w5\n",
    "            # nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, graph) :\n",
    "        # encoder\n",
    "        hv_0 = graph.x\n",
    "        hv = self.line2(hv_0)\n",
    "        # hv = F.normalize(hv) # hv / |hv|2\n",
    "        \n",
    "        total_hv = hv.unsqueeze(0).clone().detach()\n",
    "        # total_hv = torch.tensor(hv.unsqueeze(0))\n",
    "        for i in range(2, self.encoder_layers + 1) :\n",
    "            hnv = self.line5(hv, graph.edge_index)\n",
    "            hv = self.line6(hnv, hv)\n",
    "            # hv = F.normalize(hv)\n",
    "            total_hv = torch.vstack((total_hv, hv.unsqueeze(0))) # (3, 500, 64)\n",
    "        \n",
    "        z_v, _ = torch.max(total_hv, dim = 0)\n",
    "        \n",
    "        # decoder\n",
    "        out = self.decoder(z_v)\n",
    "\n",
    "        return out\n",
    "model = DrBC().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793b2ebd-a086-4821-9d41-fb62c5f13029",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a856268e-51ff-4169-9a12-7ec38dae0178",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d78c7e-c0dc-48a0-9097-9b8a43d8fd30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda.\n"
     ]
    }
   ],
   "source": [
    "device = (torch.device('cuda') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\") #可以根據輸出結果知道是否有可用的GPUa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "664117ce-32e7-47ba-92cf-b128c0491e01",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training_loop(model, n_epochs, optimizer, loss_fn, scale, batch_size = 16) : # , loss_fn, train_loader\n",
    "    ls_loss = []\n",
    "    t = tqdm(range(1, n_epochs + 1), total = n_epochs)\n",
    "    for epoch in t :   \n",
    "        start = 0\n",
    "        for i in range(batch_size) :\n",
    "            if (len(scale) == 1) : \n",
    "                s = scale[0]\n",
    "            else : \n",
    "                s = int(np.random.randint(scale[0], scale[1], 1))\n",
    "            # Draw network G from distribution D (like the power-law model)\n",
    "            g = nx.powerlaw_cluster_graph(s, 7, 0.5)\n",
    "            # [dv, 1, 1]\n",
    "            degrees = list(dict(g.degree()).values())\n",
    "            degree_tensor = torch.tensor(degrees).float().unsqueeze(1)\n",
    "            extend = torch.ones(s, 2)\n",
    "            init = torch.hstack([degree_tensor, extend])\n",
    "            edge_index = torch.tensor(list(g.edges())) + start\n",
    "            \n",
    "            # Calculate each node’s exact BC value bv, ∀v ∈ V\n",
    "            # 1-n\n",
    "            exact_bc = nx.betweenness_centrality(g)\n",
    "            exact_bc = torch.tensor(list(exact_bc.values()))\n",
    "            exact_bc = np.log(exact_bc + 1e-8)\n",
    "            \n",
    "            # 隨機取排列組合\n",
    "            input_tensor = torch.tensor(range(start, start + s))\n",
    "            combinations_tensor = torch.combinations(input_tensor, r = 2)\n",
    "            num_combinations = combinations_tensor.shape[0]\n",
    "            indices = torch.randperm(num_combinations)[:5 * s]\n",
    "            node_pair = combinations_tensor[indices]\n",
    "            \n",
    "            if (i == 0) : \n",
    "                total_init = init\n",
    "                total_exact_bc = exact_bc\n",
    "                total_node_pair = node_pair\n",
    "                total_edge_index = edge_index\n",
    "            else : \n",
    "                total_init = torch.vstack([total_init, init])\n",
    "                total_exact_bc = torch.cat([total_exact_bc, exact_bc])\n",
    "                total_node_pair = torch.vstack([total_node_pair, node_pair])\n",
    "                total_edge_index = torch.vstack([total_edge_index, edge_index])\n",
    "                \n",
    "            start += s\n",
    "            \n",
    "        idx1 = total_node_pair[::, 0]\n",
    "        idx2 = total_node_pair[::, 1]\n",
    "        pyg_g = Data(edge_index = total_edge_index.T, num_nodes = total_edge_index.max() + 1)\n",
    "        pyg_g.x = total_init\n",
    "        pred_bc = model(pyg_g.to(device = device))\n",
    "        \n",
    "        pred_a = pred_bc[idx1]\n",
    "        pred_b = pred_bc[idx2]\n",
    "        yij = pred_a - pred_b\n",
    "        exact_a = total_exact_bc[idx1]\n",
    "        exact_b = total_exact_bc[idx2]\n",
    "        bij = (exact_a - exact_b).to(device)\n",
    "        \n",
    "        yij = yij.squeeze() # (n, )\n",
    "        \n",
    "        # bij = (bij-torch.mean(bij)) / torch.std(bij)\n",
    "        # yij = (yij-torch.mean(yij)) / torch.std(yij)\n",
    "        \n",
    "        loss = loss_fn(yij, torch.sigmoid(bij))\n",
    "        ls_loss.append(loss.item())\n",
    "        # Update Θ = (ΘEN C, ΘDEC ) with Adam by minimizing Eq. (15)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "                \n",
    "        t.set_description(f'Epoch[{epoch} / {n_epochs}]')\n",
    "        t.set_postfix(loss = loss.item())\n",
    "        \n",
    "    if (len(scale) == 1) : \n",
    "        filename = './model/DrBC_{}.pt'.format(scale)\n",
    "    else : \n",
    "        filename = './model/DrBC_{}_{}.pt'.format(scale[0], scale[1])\n",
    "    torch.save(model.state_dict(), filename)\n",
    "    print(filename[8:], 'Finish Training ! ')\n",
    "    return ls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13a85773-ff41-4bab-b66e-52d2b9f70e3b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[100 / 100]: 100%|██████████| 100/100 [02:18<00:00,  1.39s/it, loss=0.517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC_100_200.pt Finish Training ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[100 / 100]: 100%|██████████| 100/100 [20:18<00:00, 12.19s/it, loss=0.53]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC_400_500.pt Finish Training ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[100 / 100]: 100%|██████████| 100/100 [1:36:50<00:00, 58.10s/it, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC_900_1000.pt Finish Training ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[100 / 100]: 100%|██████████| 100/100 [4:31:45<00:00, 163.05s/it, loss=0.534]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC_1500_1600.pt Finish Training ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch[100 / 100]: 100%|██████████| 100/100 [12:28:57<00:00, 449.37s/it, loss=0.533]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DrBC_2000_3000.pt Finish Training ! \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# train and save model\n",
    "# table 6, table 7\n",
    "learning_rate = 1e-4\n",
    "embedding_dimension = 128\n",
    "mini_batch_size = 16\n",
    "average_node_sampling_times = 5\n",
    "maximum_episodes = 10 ** 4\n",
    "layer_iterations = 5\n",
    "model = DrBC(encoder_layers = layer_iterations, emb_dim = embedding_dimension).to(device = device)\n",
    "loss_fn = nn.BCEWithLogitsLoss() # 0.693\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "n_epochs = 100\n",
    "\n",
    "train_scale = [[100, 200], [400, 500], [900, 1000], [1500, 1600], [2000, 3000]]\n",
    "\n",
    "loss = {}\n",
    "for i in train_scale :\n",
    "    ls_loss = training_loop(\n",
    "        model = model, \n",
    "        n_epochs = n_epochs, \n",
    "        optimizer = optimizer, \n",
    "        loss_fn = loss_fn, \n",
    "        scale = i, \n",
    "        batch_size = mini_batch_size\n",
    "    )\n",
    "    fig = plt.figure()\n",
    "    plt.title('Loss_scale_{}_{}'.format(i[0], i[1]))\n",
    "    temp = np.array(ls_loss)\n",
    "    plt.plot(range(n_epochs), ls_loss)\n",
    "    plt.savefig('./figure/loss_scale_{}_{}.png'.format(i[0], i[1]))\n",
    "    plt.close(fig)\n",
    "    loss[str(i)] = ls_loss\n",
    "with open(\"loss.txt\", \"w\") as fp:\n",
    "    json.dump(loss, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "35b5dc09-46fc-4352-b641-99319f863a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"loss.txt\", \"r\") as fp:\n",
    "    loss = json.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5568b272-0ffb-458e-9c36-e05798ca843e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "589788ec-9b83-42da-a50d-1b23b0c3ba95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_data(model, graph, exact_bc, top_n, device = torch.device('cpu')) :\n",
    "    s = time.time()\n",
    "    model = model.to(device)\n",
    "    pred_bc = model(graph.to(device))\n",
    "    exact_bc = np.argsort(exact_bc)\n",
    "    top = []\n",
    "    for n in top_n : \n",
    "        s = time.time()\n",
    "        num = int(np.ceil(graph.num_nodes * n / 100))\n",
    "        exact = exact_bc[-num:]\n",
    "        pred = torch.argsort(pred_bc, dim = 0)[-num:].cpu()\n",
    "        acc = len(np.intersect1d(exact, pred)) / num\n",
    "        top.append(acc)\n",
    "    t = time.time() - s\n",
    "    tau_dist, _ = stats.kendalltau(pred_bc.cpu().detach().numpy(), exact_bc)\n",
    "    top.append(tau_dist)\n",
    "    top.append(t)\n",
    "    # return np.array(top).reshape(1, -1)\n",
    "    return top"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2b38c5-ce66-4b8e-a2d7-57c83d262a8a",
   "metadata": {},
   "source": [
    "youtube\n",
    "\n",
    "|top1|top5|top10|tau|time|\n",
    "|--|--|--|--|--|\n",
    "|0.6|0.7|0.7|-0.1|0.1648|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "eb5aa0f6-4dd0-4808-b31c-60ea5da740c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy : 0.6\n",
      "Top 5 Accuracy : 0.7\n",
      "Top 10 Accuracy : 0.7\n",
      "Kendall tau distance : -0.1\n",
      "Running Time : 0.1648\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 1e-4\n",
    "embedding_dimension = 128\n",
    "maximum_episodes = 10 ** 4\n",
    "layer_iterations = 5\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "model = DrBC(encoder_layers = layer_iterations, emb_dim = embedding_dimension)\n",
    "train_scale = [[100, 200], [400, 500], [900, 1000], [1500, 1600], [2000, 3000]]\n",
    "s = train_scale[4]\n",
    "model.load_state_dict(torch.load('./model/DrBC_{}_{}.pt'.format(s[0], s[1])))\n",
    "\n",
    "# test for youtube\n",
    "top = test_data(\n",
    "    model = model, \n",
    "    graph = youtube, \n",
    "    exact_bc = youtube_score, \n",
    "    top_n = [1, 5, 10], \n",
    "    device = torch.device('cpu')\n",
    ")\n",
    "top = np.array(top)\n",
    "print('Top 1 Accuracy : {:.1f}'.format(top[0]))\n",
    "print('Top 5 Accuracy : {:.1f}'.format(top[1]))\n",
    "print('Top 10 Accuracy : {:.1f}'.format(top[2]))\n",
    "print('Kendall tau distance : {:.1f}'.format(top[3]))\n",
    "print('Running Time : {:.4f}'.format(top[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "5bab0942-2c10-470c-81a4-a23c29d09f6f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 1 Accuracy : 94.2±2.4\n",
      "Top 5 Accuracy : 89.4±1.5\n",
      "Top 10 Accuracy : 86.8±1.2\n",
      "Kendall tau distance : 34.3±0.8\n",
      "time :  2.105379104614258\n",
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# test for 30 graph\n",
    "def test_30(model, graph, score, n = 30):\n",
    "    t = []\n",
    "    top_n = [1, 5, 10]\n",
    "    for i in  range(n): \n",
    "        top = test_data(\n",
    "            model = model, \n",
    "            graph = graph[str(i)], \n",
    "            exact_bc = score[str(i)], \n",
    "            top_n = top_n, \n",
    "            device = torch.device('cpu')\n",
    "        )\n",
    "        t.append(top)\n",
    "    t = np.array(t)\n",
    "    t_mean = np.mean(t, axis = 0) * 100\n",
    "    t_std = np.std(t, axis = 0) * 100\n",
    "    print('Top',  top_n[0],  'Accuracy : {:.1f}±{:.1f}'.format(t_mean[0], t_std[0]))\n",
    "    print('Top',  top_n[1],  'Accuracy : {:.1f}±{:.1f}'.format(t_mean[1], t_std[1]))\n",
    "    print('Top',  top_n[2],  'Accuracy : {:.1f}±{:.1f}'.format(t_mean[2], t_std[2]))\n",
    "    print('Kendall tau distance : {:.1f}±{:.1f}'.format(t_mean[3], t_std[3]))\n",
    "    print('time : ', t_mean[4] * n)\n",
    "    return t_mean, t_std\n",
    "mean, std = test_30(\n",
    "    model = model, \n",
    "    graph = total_graph, \n",
    "    score = total_score, \n",
    "    n = 30\n",
    ")\n",
    "print(type(mean))\n",
    "print(type(std))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "4de813ba-df0f-45a2-affb-3f3d93a8e62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_graph(nodes) :\n",
    "    g = nx.powerlaw_cluster_graph(nodes, 7, 0.5)\n",
    "    # [dv, 1, 1]\n",
    "    degrees = list(dict(g.degree()).values())\n",
    "    degree_tensor = torch.tensor(degrees).float().unsqueeze(1)\n",
    "    extend = torch.ones(nodes, 2)\n",
    "    init = torch.hstack([degree_tensor, extend])\n",
    "    edge_index = torch.tensor(list(g.edges()))\n",
    "\n",
    "    # Calculate each node’s exact BC value bv, ∀v ∈ V\n",
    "    # 1-n\n",
    "    exact_bc = nx.betweenness_centrality(g)\n",
    "    exact_bc = torch.tensor(list(exact_bc.values()))\n",
    "    exact_bc = np.log(exact_bc + 1e-8)\n",
    "    pyg_g = Data(edge_index = edge_index.T, num_nodes = edge_index.max() + 1)\n",
    "    pyg_g.x = init\n",
    "    \n",
    "    return pyg_g, exact_bc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "fabf42d3-1fdc-462f-818c-9d37e3446e24",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:01<00:00, 26.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scale : 100_200\n",
      "test_scale : 100\n",
      "Top 1 Accuracy : 93.3±24.9\n",
      "Top 5 Accuracy : 96.0±8.0\n",
      "Top 10 Accuracy : 96.0±4.9\n",
      "Kendall tau distance : 38.6±5.7\n",
      "time :  0.5038976669311523\n",
      "\n",
      "train scale : 400_500\n",
      "test_scale : 100\n",
      "Top 1 Accuracy : 93.3±24.9\n",
      "Top 5 Accuracy : 94.0±9.2\n",
      "Top 10 Accuracy : 95.7±5.6\n",
      "Kendall tau distance : 39.7±5.5\n",
      "time :  0.49412250518798834\n",
      "\n",
      "train scale : 900_1000\n",
      "test_scale : 100\n",
      "Top 1 Accuracy : 93.3±24.9\n",
      "Top 5 Accuracy : 92.0±11.1\n",
      "Top 10 Accuracy : 94.3±5.6\n",
      "Kendall tau distance : 40.8±5.4\n",
      "time :  0.49703121185302734\n",
      "\n",
      "train scale : 1500_1600\n",
      "test_scale : 100\n",
      "Top 1 Accuracy : 93.3±24.9\n",
      "Top 5 Accuracy : 88.0±12.2\n",
      "Top 10 Accuracy : 92.3±6.2\n",
      "Kendall tau distance : 40.9±5.4\n",
      "time :  0.4973888397216797\n",
      "\n",
      "train scale : 2000_3000\n",
      "test_scale : 100\n",
      "Top 1 Accuracy : 93.3±24.9\n",
      "Top 5 Accuracy : 81.3±12.6\n",
      "Top 10 Accuracy : 90.0±6.3\n",
      "Kendall tau distance : 40.9±5.1\n",
      "time :  0.5003452301025391\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [00:27<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scale : 100_200\n",
      "test_scale : 500\n",
      "Top 1 Accuracy : 96.7±7.5\n",
      "Top 5 Accuracy : 96.4±3.6\n",
      "Top 10 Accuracy : 94.9±2.0\n",
      "Kendall tau distance : 36.3±2.7\n",
      "time :  0.8106470108032225\n",
      "\n",
      "train scale : 400_500\n",
      "test_scale : 500\n",
      "Top 1 Accuracy : 96.7±7.5\n",
      "Top 5 Accuracy : 96.1±3.2\n",
      "Top 10 Accuracy : 94.6±1.7\n",
      "Kendall tau distance : 37.1±2.7\n",
      "time :  0.8044958114624023\n",
      "\n",
      "train scale : 900_1000\n",
      "test_scale : 500\n",
      "Top 1 Accuracy : 97.3±6.8\n",
      "Top 5 Accuracy : 95.6±2.8\n",
      "Top 10 Accuracy : 94.7±1.9\n",
      "Kendall tau distance : 38.3±2.8\n",
      "time :  0.7380008697509766\n",
      "\n",
      "train scale : 1500_1600\n",
      "test_scale : 500\n",
      "Top 1 Accuracy : 97.3±6.8\n",
      "Top 5 Accuracy : 94.9±3.3\n",
      "Top 10 Accuracy : 94.7±1.9\n",
      "Kendall tau distance : 37.7±2.7\n",
      "time :  0.7692337036132812\n",
      "\n",
      "train scale : 2000_3000\n",
      "test_scale : 500\n",
      "Top 1 Accuracy : 97.3±6.8\n",
      "Top 5 Accuracy : 94.1±3.5\n",
      "Top 10 Accuracy : 94.3±1.9\n",
      "Kendall tau distance : 37.4±2.7\n",
      "time :  0.7283210754394531\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [01:57<00:00,  3.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scale : 100_200\n",
      "test_scale : 1000\n",
      "Top 1 Accuracy : 97.0±5.3\n",
      "Top 5 Accuracy : 94.5±1.8\n",
      "Top 10 Accuracy : 91.9±2.2\n",
      "Kendall tau distance : 37.0±1.7\n",
      "time :  1.387834548950195\n",
      "\n",
      "train scale : 400_500\n",
      "test_scale : 1000\n",
      "Top 1 Accuracy : 97.3±5.1\n",
      "Top 5 Accuracy : 94.9±1.8\n",
      "Top 10 Accuracy : 92.8±1.9\n",
      "Kendall tau distance : 37.7±1.7\n",
      "time :  0.9442567825317384\n",
      "\n",
      "train scale : 900_1000\n",
      "test_scale : 1000\n",
      "Top 1 Accuracy : 97.3±5.1\n",
      "Top 5 Accuracy : 95.7±1.7\n",
      "Top 10 Accuracy : 92.9±2.1\n",
      "Kendall tau distance : 38.5±1.7\n",
      "time :  0.9584426879882812\n",
      "\n",
      "train scale : 1500_1600\n",
      "test_scale : 1000\n",
      "Top 1 Accuracy : 97.3±5.1\n",
      "Top 5 Accuracy : 95.9±1.7\n",
      "Top 10 Accuracy : 93.5±1.8\n",
      "Kendall tau distance : 37.5±1.8\n",
      "time :  0.9268760681152343\n",
      "\n",
      "train scale : 2000_3000\n",
      "test_scale : 1000\n",
      "Top 1 Accuracy : 97.3±5.1\n",
      "Top 5 Accuracy : 95.8±1.7\n",
      "Top 10 Accuracy : 93.7±1.8\n",
      "Kendall tau distance : 37.1±1.8\n",
      "time :  1.0218143463134766\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [08:06<00:00, 16.23s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scale : 100_200\n",
      "test_scale : 2000\n",
      "Top 1 Accuracy : 96.7±2.7\n",
      "Top 5 Accuracy : 92.9±1.7\n",
      "Top 10 Accuracy : 90.8±1.1\n",
      "Kendall tau distance : 36.5±1.3\n",
      "time :  1.2679338455200195\n",
      "\n",
      "train scale : 400_500\n",
      "test_scale : 2000\n",
      "Top 1 Accuracy : 96.8±2.7\n",
      "Top 5 Accuracy : 93.5±1.7\n",
      "Top 10 Accuracy : 91.2±1.1\n",
      "Kendall tau distance : 37.3±1.3\n",
      "time :  1.511120796203613\n",
      "\n",
      "train scale : 900_1000\n",
      "test_scale : 2000\n",
      "Top 1 Accuracy : 96.7±3.0\n",
      "Top 5 Accuracy : 94.2±1.5\n",
      "Top 10 Accuracy : 91.8±1.0\n",
      "Kendall tau distance : 37.9±1.2\n",
      "time :  1.2858390808105469\n",
      "\n",
      "train scale : 1500_1600\n",
      "test_scale : 2000\n",
      "Top 1 Accuracy : 96.8±2.7\n",
      "Top 5 Accuracy : 94.3±1.6\n",
      "Top 10 Accuracy : 92.8±1.0\n",
      "Kendall tau distance : 36.6±1.2\n",
      "time :  1.3888835906982422\n",
      "\n",
      "train scale : 2000_3000\n",
      "test_scale : 2000\n",
      "Top 1 Accuracy : 97.0±2.8\n",
      "Top 5 Accuracy : 94.5±1.6\n",
      "Top 10 Accuracy : 93.4±1.0\n",
      "Kendall tau distance : 36.0±1.2\n",
      "time :  1.396059989929199\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [1:06:20<00:00, 132.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train scale : 100_200\n",
      "test_scale : 5000\n",
      "Top 1 Accuracy : 95.2±1.8\n",
      "Top 5 Accuracy : 90.4±1.6\n",
      "Top 10 Accuracy : 87.4±1.1\n",
      "Kendall tau distance : 36.1±0.6\n",
      "time :  2.379536628723145\n",
      "\n",
      "train scale : 400_500\n",
      "test_scale : 5000\n",
      "Top 1 Accuracy : 95.9±1.8\n",
      "Top 5 Accuracy : 90.6±1.5\n",
      "Top 10 Accuracy : 87.8±1.0\n",
      "Kendall tau distance : 36.8±0.6\n",
      "time :  2.232027053833008\n",
      "\n",
      "train scale : 900_1000\n",
      "test_scale : 5000\n",
      "Top 1 Accuracy : 95.7±1.7\n",
      "Top 5 Accuracy : 91.7±1.6\n",
      "Top 10 Accuracy : 88.8±0.9\n",
      "Kendall tau distance : 37.3±0.7\n",
      "time :  2.2801876068115234\n",
      "\n",
      "train scale : 1500_1600\n",
      "test_scale : 5000\n",
      "Top 1 Accuracy : 96.0±1.9\n",
      "Top 5 Accuracy : 92.3±1.7\n",
      "Top 10 Accuracy : 90.3±0.9\n",
      "Kendall tau distance : 35.8±0.7\n",
      "time :  2.273273468017578\n",
      "\n",
      "train scale : 2000_3000\n",
      "test_scale : 5000\n",
      "Top 1 Accuracy : 96.2±2.0\n",
      "Top 5 Accuracy : 92.5±1.7\n",
      "Top 10 Accuracy : 91.3±0.8\n",
      "Kendall tau distance : 35.1±0.8\n",
      "time :  2.3049354553222656\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# different scale\n",
    "# not yet\n",
    "learning_rate = 1e-4\n",
    "embedding_dimension = 128\n",
    "maximum_episodes = 10 ** 4\n",
    "layer_iterations = 5\n",
    "loss_fn = nn.BCEWithLogitsLoss(reduction = 'sum')\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate, weight_decay = 1e-5)\n",
    "model = DrBC(encoder_layers = layer_iterations, emb_dim = embedding_dimension)\n",
    "train_scale = [[100, 200], [400, 500], [900, 1000], [1500, 1600], [2000, 3000]]\n",
    "test_scale = [100, 500, 1000, 2000, 5000]\n",
    "\n",
    "\n",
    "def diff_scale(train_scale, test_scale) :\n",
    "    acc_mean = {}\n",
    "    acc_std = {}\n",
    "    for j in test_scale :\n",
    "        # generate 30 graoh\n",
    "        total_test = {}\n",
    "        total_test_bc = {}\n",
    "        for k in trange(30) :\n",
    "            pyg, exact_bc = gen_graph(j)\n",
    "            total_test[str(k)] = pyg\n",
    "            total_test_bc[str(k)] = exact_bc\n",
    "        for i in train_scale :\n",
    "            print('train scale : {}_{}'.format(i[0], i[1]))\n",
    "            print('test_scale : {}'.format(j))\n",
    "            model.load_state_dict(torch.load('./model/DrBC_{}_{}.pt'.format(i[0], i[1])))\n",
    "            mean, std = test_30(\n",
    "                model = model, \n",
    "                graph = total_test, \n",
    "                score = total_test_bc, \n",
    "                n = 30\n",
    "            )\n",
    "            print('')\n",
    "            idx = '{}_{}_{}'.format(i[0], i[1], j)\n",
    "            acc_mean[idx] = list(mean)\n",
    "            acc_std[idx] = list(std)\n",
    "    \n",
    "    return acc_mean, acc_std\n",
    "            \n",
    "acc_mean, acc_std = diff_scale(train_scale, test_scale)  \n",
    "with open(\"acc_mean.txt\", \"w\") as fp:\n",
    "    json.dump(acc_mean, fp)\n",
    "with open(\"acc_std.txt\", \"w\") as fp:\n",
    "    json.dump(acc_std, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "19b00690-b753-4b36-9bde-9b5560148527",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"acc_mean.txt\", \"r\") as fp:\n",
    "    acc_mean = json.load(fp)\n",
    "with open(\"acc_std.txt\", \"r\") as fp:\n",
    "    acc_std = json.load(fp)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21aca1d2-7723-4dbb-9b97-f7c53c44cc6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "27580a6d-b5c5-4322-94c6-cbca90086ced",
   "metadata": {},
   "source": [
    "### TODO List\n",
    "1. INPUT 只要放BC值\n",
    "2. TOP N ARGSORT了兩次\n",
    "3. diff_scale 未完成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84a753ab-b833-48e4-b877-4d53555f8790",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8b3a38-22a3-48fb-94f3-4eeede986d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyterlab",
   "language": "python",
   "name": "jupyterlab"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
